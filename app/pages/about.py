import streamlit as st

st.set_page_config(page_title="About Â· Insight AI", layout="centered")

st.title("ðŸ§  About Insight AI")

st.markdown(
    """
    **Insight AI** is an interactive, explainable computer vision application designed to bridge the gap between
    *model predictions* and *human understanding*.

    Users can upload an image and receive:
    - ðŸ” A **model prediction** from a pretrained convolutional neural network (CNN)
    - ðŸ–¼ï¸ A **visual explanation** using Grad-CAM heatmaps
    - ðŸ“ A **natural-language description** generated by a BLIP visionâ€“language model
    - ðŸ’¡ A **human-friendly insight layer** that maps captions and predictions to interpretable concepts

    In addition to explainability, the application incorporates a **human-in-the-loop feedback mechanism** that
    allows users to validate and correct both predictions and captions. This feedback is persisted and used to
    influence future inference behavior without modifying model weights.
    """
)

st.divider()

st.subheader("ðŸ”§ Architecture Overview")

st.markdown(
    """
    The application is structured as a modular, production-aware ML system:

    1. **Image Preprocessing**  
       Uploaded images are resized and normalized for compatibility across models.

    2. **CNN Prediction**  
       A pretrained CNN produces class probabilities for the input image.

    3. **Grad-CAM Explainability**  
       Gradient-weighted class activation maps highlight regions of the image
       that most influenced the modelâ€™s prediction.

    4. **BLIP Captioning (Vision â†’ Language)**  
       A BLIP visionâ€“language model generates a natural-language description
       of the image content.

    5. **Insight Mapping Layer**  
       Captions and predictions are mapped to domain-relevant keywords to
       produce human-readable explanations.

    6. **User Feedback Loop**  
       Users validate predictions and captions, optionally correct labels,
       and provide comments. Feedback is logged and used to dynamically
       influence future inference behavior via a lightweight mapping layer,
       without retraining the underlying models.
    """
)

st.divider()

st.subheader("ðŸ” Human-in-the-Loop Design")

st.markdown(
    """
    Rather than retraining models online, Insight AI captures **structured supervision at inference time**.

    - Prediction correctness is validated by the user
    - Caption quality is independently evaluated
    - Corrections are persisted in JSON and CSV logs
    - Future sessions benefit from prior feedback through dynamic mapping

    This design prioritizes **safety, auditability, and explainability** while
    preserving the stability of deployed models.
    """
)

st.divider()

st.subheader("ðŸš€ Tech Stack")

st.markdown(
    """
    - **Frontend:** Streamlit  
    - **Vision Model:** CNN (TensorFlow / Keras)  
    - **Explainability:** Grad-CAM  
    - **Visionâ€“Language Model:** BLIP (Hugging Face Transformers)  
    - **Feedback Storage:** JSON (dynamic mappings), CSV (audit log)  
    - **Deployment:** Streamlit Community Cloud (CPU-only)
    """
)

st.divider()

st.subheader("ðŸ“Œ Project Motivation")

st.markdown(
    """
    Many machine learning applications stop at prediction accuracy.
    Insight AI goes a step further by asking:

    > *Why did the model make this decision â€” and how can a human meaningfully respond to it?*

    By combining visual explanations, language-based reasoning, and a
    human-in-the-loop feedback mechanism, this project demonstrates how
    explainable AI systems can be both **transparent** and **interactive**
    in real-world deployment settings.
    """
)
st.caption("Author: Sheron Schley / github.com/O-S-O-K")
st.caption("Â© Insight AI Â· Explainable Vision with Human-in-the-Loop Feedback")
